\section{Methodology}
\label{back_meth}
This section describes technical details of the methods employed in this dissertation. We provide details on convolutional and attention-based neural models, describe zero-shot learning paradigm and document ranking task, define the metrics for model evaluation.

\subsection{Neural networks}
A neural network performs a series of transformations of the input data to get the desired output. The work unit in the neural network, a perceptron, computes a weighted sum of the input vector $x$:
\begin{equation}
    z = f(Wx + b)
\end{equation}
where $W$ and $b$ are the network parameters, which are updated during training. $f$ is the \textit{activation function}, adding non-linearity to the outputs. Popular choices for the activation function are \textit{sigmoid} and \textit{Rectified Linear Unit} (ReLU) functions:
\vspace{-0.1pt}
\begin{gather}
    sigmoid(z) = \frac{1}{1+e^{-z}} \\
    ReLU(z) = max(0, z) 
\end{gather}

The neural network is comprised of multiple perceptrons, arranged in layers, performing sequential operations on the input. The outputs from the last layer of the network serve as the predictions of the model. For example, in multi-class classification the network's outputs can represent the scores for each predicted class. To transform the scores into probabilities of each class the \textit{softmax} function is used:

\begin{equation}
   softmax(y_i) = \frac{e^{y_i}}{\sum_j e^{y_j}}
\end{equation}

During training the network learns how to reproduce the true values $\hat{y}$ by adjusting the trainable parameters $W$ of each perceptron. To achieve that, the network's output $y$ is compared with the desired $\hat{y}$ using a \textit{loss function}, which evaluates how different are $y$ and $\hat{y}$. In classification problems the most commonly used loss function is the \textit{cross entropy} loss:
\begin{equation}
    L = - \frac{1}{m}\sum_{i=1}^m \hat{y}_i \log(y_i)
\end{equation}
where $m$ is the number of classes in the classification problem. The objective of neural network training is to minimize the loss function.

The process of adjusting the parameters of the network layer by layer is called \textit{backpropagation}. The parameters are updated in the direction of loss minimisation by taking partial derivatives:
\begin{equation}
    w' = w - \alpha\frac{\delta L}{\delta w}
\end{equation}
where $\alpha$ is the \textit{learning rate}, the speed of changing the weights. Learning rate is a hyperparameter of the network, which does not get updated in the backpropagation and needs to be carefully selected. Very high learning rates lead to parameters oscillating around the optimal values, while very small learning rates cause slow training convergence.

\subsection{Word embeddings}

Textual data used in natural language processing tasks can not be directly processed by neural networks, which require numerical data as input. To overcome this, usually each word in the vocabulary is encoded into a real-valued vector representation, called a \textit{word embedding}. Word embeddings capture word's semantic information, so that the words which are synonymous or are used in similar contexts will have similar representations (in terms of cosine similarity of the corresponding embedding vectors).

Word embeddings can be learned jointly with neural network training. In this case each words is associated with a random real-valued vector, which is updated during the backpropagation in the network to capture task-specific work characteristics. Another popular option is to use pretrained word embeddings, learned by some external model.

One popular choice of pretrained word embeddings is \textit{Word2Vec} \cite{mikolov2013efficient} method, which preserves contextual information of the terms. Word2Vec embeddings are trained on one of the tasks: (i) continuous bag of words (predicting the current term from its context) or (ii) skip-gram model (predicting surrounding terms given the current word). 

The limitation of using pretrained Word2Vec embeddings is that the final embedding for a term is the same for all contexts it can occur in, which can cause problems with the polysemous words (like `\textit{bank}' or `\textit{lie}'). Thus, recently the \textit{contextualized} embedding models, such as BERT, described in Section \ref{bert}, have recently gained considerable popularity. These models are pretrained on a large linguistic corpus, which enables them to statistically learn the structure of language. They produce the embeddings for the whole sequence simultaneously, effectively capturing terms' context. 

\subsection{Convolutional Neural Networks}

A Convolutional Neural Network (CNN) \cite{lecun1989backpropagation} is a neural architecture, which is tailored for the tasks, where absolute positions in the input do not matter, i.e. the properties of subparts of the inputs to a CNN should be invariant to shifts. An example of such application is spam detection, where the task is to check if there are any suspicious phrases in the input, but the exact absolute position of such a phrase does not matter. 

The input to a CNN is a word sequence $x_{1:n}$ of length $n$, where each word is represented by a $k$-dimensional embedding, $x_i \in \mathbb{R}^k$. The CNN multiplies a weight matrix $w \in \mathbb{R}^{hk}$, called a \textit{convolutional filter}, to the slices of the input of length $h$. After applying the filter to each possible slice in the input, we get a \textit{convolutional feature map}:
\begin{gather*}
    \textbf{c} = [c_1, ..., c_{n-h+1}], \\
    c_i = f(w \times x_{i:i+h-1} + b)
\end{gather*}

where $x_{i:i+h-1}$ denotes a slice between $i$-th and and $i+h-1$-th words, $f$ is the activation function, $c_i$ is a feature produced by a single filter application. Effectively the filter $w$ detects if a $h$-sized slice has a particular feature (for instance, it can detect if a bigram $x_{i:i+1}$ is a pair ``\textit{verb + noun}''). The features in the feature map \textbf{c} are collected with some aggregation operation, for example max-pooling $\hat{\textbf{c}} = max{(\textbf{c})}$, which acts as a representative for the used filter (e.g. $\hat{\textbf{c}}$ can show the maximum probability score of having spam in some input slice). On each network layer multiple filters are applied to detect various features, capturing different aspects of the input.

Having gained popularity in computer vision, CNNs have also shown strong results in natural language tasks, such as sentence classification or sentiment prediction \cite{kim-2014-convolutional, kalchbrenner2014convolutional, collobert2011natural}. Their advantages are a small number of parameters (the same small weight matrix is applied to each input slice), fast computation and good interpretability. However, the contexts considered by CNNs are limited by the selected convolutional filter size, therefore CNNs can not properly model long-term word dependencies.

\subsection{Attention models}

Attention mechanisms have been introduced to represent the long range contextual information more adequately, as compared to convolutional or recurrent architectures \cite{bahdanau2014neural}. Attention assigns weights to the input elements, indicating which inputs to focus on to make a correct prediction. In natural language processing, attention mechanism is used to produce representations for sequence pairs (for example, in language generation) or single sentences (e.g., for sentence classification tasks).

Given as input a vector $s$ and a context $h_1, ... h_n$, the attention model creates a refined representation of $s$ by incorporating the information from the context, based on the relevance of each context element $h_i$ to the given $s$. For each $h_i$ self-attention computes an \textit{attention weight} $\alpha_i_j$ as follows:
\begin{spreadlines}{10pt} 
\begin{gather*}
    \alpha_i = \frac{exp(e_i)}{\sum_{k=1}^n exp(e_k)} \\ 
    e_i = f(s, h_i)
\end{gather*}
\end{spreadlines}

where $n$ is the length of context sequence and $f$ is an attention function. $f$ can be implemented with a dot-product, sum or be computed with a neural network.

Then the attention context vector for $s$ will be a sum of the context elements $h_i$ weighted by their attention scores:
\begin{equation}
    c = \sum_{i=1}^n \alpha_i h_i 
\end{equation}

The context vector $c$ can be used to augment the representation of $s$, for example, by concatenating them. 
Attention models were shown to work well for many natural language tasks, where distant contextual information is important \cite{bahdanau2014neural, yin2016abcnn}. Moreover, attention computation can be effectively parallelized, as opposed to computations in sequential models. Attention weights are also highly interpretable, allowing to investigate which sequence elements were influential for creating contextual representations.

\subsubsection{Transformer encoder}
\textit{Transformer} \cite{vaswani2017attention} is a state-of-the-art neural model based on attention mechanisms. Transformer is a sequence-to-sequence model, used to convert the input sequence to the output sequence (for example, it can be used for machine translation). The use of attention mechanisms allows Transformer to produce bidirectional input representations - capturing both left and right context of the tokens, as opposed to unidirectional representations generated by recurrent models.

Transformer consists of an \textit{encoder}, for creating the representation of the input sequence, and a \textit{decoder}, for generating the output. The encoder in Transformer can also be used as a standalone model for creating refined input sequence embeddings. In the following we will provide a description of the architecture of Transformer encoder.  

One Transformer encoder module consists of two blocks: a multi-head self-attention and a fully-connected feed forward neural network. Each block is surrounded by a residual connection \cite{he2016deep} (which adds the unchanged input to the output of the block), which is followed by layer normalization \cite{ba2016layer}. Transformer encoder is composed of several such modules stacked.

The input elements to the encoder are summed with sinusoidal positional encoding, since attention mechanism does not preserve the information about the absolute positions. Self-attention function in Transformer, called \textit{Scaled Dot-Product Attention}, estimates the compatibility of each word in the sequence (denoted as query Q) to the rest of the words (keys K), which forms coefficients in the weighted sum of the values (V):

\begin{equation}
    attention(Q,V,K) = V \cdot softmax(\frac{QK^T}{\sqrt{d_k}})
\end{equation}
 
where $d_k$ is the dimentionality of keys K, which scales the dot-product. In Transformer encoder, Q, K, V are all the representations of the input elements.

To capture the information from different subspaces from the input, in Transformer several attention functions are performed in parallel on different input projections, which is called \textit{muli-head attention}. The Q, K, V matrices are linearly projected $h$ times, where $h$ is the number of attention heads, and attention function is applied to each projection:

\begin{equation}
    head_i = attention(QW_i^Q,KW_i^K,VW_i^V).
\end{equation}

where $W_i$ are projection matrices. All heads are then concatenated and projected again:

\begin{equation}
    MultiHead(Q, K, V) = concat(head_1,...head_h)W^O.
\end{equation}

where matrix $W^O$ maps from the projected dimension $h \cdot d_k$ back to the original dimension $d_k$. 

The second block of the encoder is a two layer position-wise feed forward network with a ReLU activation between the layers:

\begin{equation}
    ffn(x) = ReLU(xW_1+b_1)W_2+b_2
\end{equation}
 
After the last encoder module the output representation can be used in further modules, such as Transformer decoder or a classification layer.

\subsubsection{Bidirectional Encoder Representations from Transformers (BERT)}
\label{bert}

BERT \cite{devlin2018bert} is a model for creating deep contextualized representations of the input text. BERT is usually pretrained on a very large unlabeled text corpus and then can be fine-tuned for inference on other natural language tasks.

BERT is comprised of an embedding layer, several stacked Transformer encoder layers and a task-specific output layer. BERT is pretrained on two tasks: (i) masked language modelling and (ii) next sentence prediction.

In the masked language modelling task some tokens in the input are replaced with a special [MASK] token. Given the information from the context of the masked input, BERT predicts the original value of the word, by producing the probability distribution over vocabulary. In the next sentence prediction task BERT is trained on pairs sentences, trying to classify whether the second sentence was subsequent to the first one in the original document. Both tasks don't need explicit labels in the corpus, making pretraining of BERT semi-supervised.

BERT was shown to achieve state-of-the-art results on many NLP tasks \cite{devlin2018bert}, still it has several limitations. The number of trainable parameters in BERT is very large (110 million parameters in the BERT$_{base}$ model and 345 million in BERT$_{large}$), which makes training it on a large corpus very time-consuming. Moreover BERT's input can't exceed 512 tokens, which is too few for many tasks dealing with long texts. Trying to overcome this limitation leads to splitting or cropping the input, which causes loss of information.

\subsection{Zero-shot Learning}

The term zero-shot learning refers to the models making predictions over the set of labels, which may not have been observed during training time. Zero-shot learning is applied to the problems with scarce training data, where the labeled training instances cover only few of the possible classes and test data can potentially contain unobserved classes. One example of such problem is classification in a dynamic environment, where new classes appear within time; another example is very fine-grained classification into a huge number of classes, where capturing sufficient training samples for each of them is infeasible.

Usually zero-shot problems are solved by mapping training and test instances to some common latent representation space, where classification can be done easier. This mapping is learnt from the observed classes at training and is applied to the zero-shot classes at test time. %In early research the objects were mapped using hand-crafted features, such as human-generated textual attributes for the images. This is a tedious and expensive approach, therefore, more recent architectures learn the features used for mapping.

%The body of research on zero-shot learning for natural language tasks is scarce. \citet{li2018deep} solve the task of zero-shot \textit{document filtering} by learning relevance between categories and documents, which are represented with category-dependent embeddings. \citet{dauphin2013zero} investigates zero-shot \textit{semantic utterance classification}, by mapping the utterances and potentially unseen categories into same semantic space, where they can be matched with distance functions.

%Other zero-shot models solving natural language problems include the ones for \textit{relation extraction} \cite{levy2017zero}, where the relations serve as unseen classes and their instances are recovered by casting the relations to natural language questions and reducing the problem to reading comprehension; and \textit{entity extraction} \cite{pasupat2014zero}, where the extraction of entities from the web documents is done with a text query, removing the need for specifying a set of seed terms.

\subsection{Document ranking}
\label{back_ir}

Information Retrieval (IR) addresses the task of searching through the document collection and retrieving the documents, which are relevant to the given query. It involves creating representations of both the documents (known as \textit{indexing}) and the user's query, and defining a matching function between these representations, called a \textit{ranking model}. 

Indexing involves such operations as stemming, removing stop-words or creating inverted index (the mapping from each vocabulary word to the documents it occurs in and the frequency of occurrence). 

The ranking model outputs the optimal ranking of the documents with respect to the given query. Ranking models range from boolean, vector or probabilistic methods to neural models. In this section we give an overview of two ranking models we used in our research.

\textit{BM25} \cite{robertson1995okapi} is a popular probabilistic ranking model, based on the frequencies of query terms appearing in each document. For a query $Q=q_1,... q_n$ and the document $D$, BM25 produces a score:

\begin{equation}
    score(D, Q) = \sum_{i=1}^n IDF(q_i) \frac{f(q_i,D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 (1 - b + b\frac{|D|}{avg\_doc\_len})}
\end{equation}

where $f(q_i, D)$ is the frequency of occurrence of the term $q_i$ in the document $D$, $|D|$ is the length of the document $D$ and $avg\_doc\_len$ is the average length of the document in the collection. The parameters $k_1$ and $b$ can chosen freely. $k_1$ parameter regulates how much the document score can be affected by a single term; $b$ controls the impact of the relative document length. $IDF(q_i)$ is the inverse document frequency of the term $q_i$, which can be calculated as:

\begin{equation}
    IDF(q_i) = ln \left ( \frac{n - n(q_i)+0.5}{n(q_i) + 0.5} + 1 \right )
\end{equation}

where $N$ is the number of documents in the collection, $n(q_i)$ is the number of documents containing $q_i$. Inverse document frequency effectively assigns lower scores to the terms that are frequent in the document collection.

BM25 formula is intuitive and shows good results, even compared to the state-of-the-art neural approaches. However, it requires exact term matches and does not consider the order of the words in the query and the document.

K-NRM \cite{xiong2017end} is a neural ranking model with takes advantage of embedding similarity between query and document representations, addressing the limitations of the models with exact term matches. K-NRM creates a translation matrix, containing the similarity between each pair of query and document words. After that a kernel-pooling technique is applied to extract soft match features (soft count of word pairs frequencies at multiple similarity levels), which are used to produce the final ranking score. The kernel method helps to deal with the imprecise word matches, caused by calculating word similarity at a single level.

\subsection{Evaluation metrics}

In this section we discuss classification and ranking quality metrics used for model evaluation in our experiments. Depending on the nature of the predicted attribute (binary or multi-class; single- or multi-label) we selected corresponding metrics. 

\paragraphHd{Accuracy, recall, precision, F1.} We first give the definitions for the classification metrics in the basic binary case (positive and negative classes):

\begin{spreadlines}{15pt}  
\begin{gather}
    accuracy = \frac{\text{num correct predictions}}{\text{num test instances}} \\ 
    precision = \frac{TP}{TP + FP} \quad \quad recall = \frac{TP}{TP + TN}
\end{gather}
\end{spreadlines}


where $TP$, \textit{true positives}, is the number of true labels that the model correctly predicted; $TN$, \textit{true negatives}, is the number of true labels that the model failed to predict; $FP$, \textit{false positives} is the number of incorrect models' predictions. Precision shows how accurate model's predictions are, recall shows how many true labels the model could identify. 

The output of the binary classifier depends on the score of the positive class: if the score exceeds a particular \textit{decision threshold}, the positive class is predicted; otherwise the negative class. If the model score classes with probabilities, the default threshold is usually set at $0.5$; yet, depending on the model architecture and loss function used for training, it may be beneficial to tune the exact value of the threshold. Varying decision threshold results in changing $TP$ and $FN$ error counts (changing the number of instances to be classified as positive), which causes one of precision or recall metrics to increase and the other one to decrease. It is therefore useful to calculate their harmonic mean:

\begin{equation}
    F1 = \frac{2 \cdot precision \cdot recall}{(precision + recall)}
\end{equation}

\paragraphHd{AUROC.} A \textit{Receiver Operating Characteristic} (ROC) curve is a plot depicting the performance of the binary classifier at varying decision threshold values. ROC curve is plotted in true positive rate ($TPR$) vs. false positive rate ($FPR$) coordinates. $TPR$ is equal to recall; $FPR$ is calculated as:

\begin{equation}
    FPR = \frac{FP}{FP + TN}
\end{equation}

AUROC metrics computes the area under ROC curve. It can be interpreted as an expectation that a randomly drawn positive example will be ranked higher than a randomly drawn negative example. AUROC is a binary classification metrics and its extension to the multi-class case requires binarizing the predictions.

\paragraphHd{Multi-class metrics.} For multi-class predictions precision, recall and F1 metrics can be computed using \textit{micro} or \textit{macro} averaging. In case of micro averaging the metrics are calculated globally across all classes; macro averaging implies calculating metrics per class and averaging them. For example, the equations for micro and macro precision for classification with $N$ classes will look like:

\begin{align}
    precision_{micro} = \frac{\sum_{c}^N TP_c}{\sum_{c}^N TP_c + \sum_{c}^N FP_c} \\
    precision_{macro} = \frac{1}{N}\sum_{c}^N \frac{TP_c}{TP_c + FP_c}
\end{align}

A \textit{confusion matrix} is an $N\times N$ matrix which shows for each pair of classes the number of misclassifications between them. The rows correspond to the true labels and the columns to the predicted labels; the number in the $ij$-th matrix cell denotes the number of times the model predicted class $j$, when the correct label was class $i$. Confusion matrix is useful for qualitative model analysis, allowing to see systematic misclassifications.

\subsubsection{Ranking metrics}

Multi-class classification can also be viewed as a ranking problem, where the aim is to assign the highest rank to the correct label in the list of class scores outputted by the model. 

\paragraphHd{nDCG.} A normalized Discounted Cumulative Gain (nDCG) measures the gain of placing each label on its position in the ranked list of model's predictions, based on labels' relevance. For classification problem we can set the relevance of all correct labels to 1 and 0 for incorrect ones. A Cumulative Gain at rank $r$ (CG$_r$) is calculated as:

\begin{equation}
    CG_r = \sum_{i=1}^r rel_i
\end{equation}

where $rel_i$ is the relevance of the class at the position $i$ in the ranked list of predictions. CG only calculates the total relevance of predictions up to position $r$, irrespective of their order. The Discounted Cumulative Gain (DCG) reduces the relevance impact of the correct predictions logarithmically proportional to their position in the list:

\begin{equation}
    DCG_r = \sum_{i=1}^r \frac{rel_i}{log_2(i+1)}
\end{equation}

A normalized Discounted Cumulative Gain (nDCG) is a fraction of the achieved DCG to the ideal one:

\begin{align}
    nDCG_r = \frac{DCG_r}{IDCG_r} && IDCG_r = \sum_{i=1}^{|rel_r|} \frac{rel_i}{log_2(i+1)}
\end{align}

where $IDCG_r$ is an ideal DCG, calculated on the ranked list of only relevant results up to the position $r$. For a perfect ranking model $nDCG_r = 1$.

\paragraphHd{MRR.} The mean reciprocal rank (MRR) is the ranking metrics, calculating the average of the inverse ranks of correct predictions in the sorted list of results:

\begin{equation}
    MRR = \frac{1}{N} \sum_{i=1}^N \frac{1}{rank_i}
\end{equation}

where $N$ is the number of test instances. MRR considers only the rank of the first correct label, thus it is not applicable to multi-label classification.
