\section{Related Work}

The models HAMs and CHARM, described in the previous two chapters, make predictions based on the input from a single speaker. Meanwhile, relationship inference requires processing the utterances of a pair of interlocutors in a conversation.
In this section we will summarize related work on modeling multi-speaker dialogues. Many natural language processing tasks based on conversational speech (chatbot answer generation, utterance intent classification, emotion prediction, etc.) require creating a representation of a given multi-speaker conversation as input.
We identify several features typical of the conversational data, which can be used to enhance predictive models: \textit{(i)} conversational structure, \textit{(ii)} speaker attribution, \textit{(iii)} additional speaker information. 

\paragraph{Conversational structure.} 
One popular way to represent a conversation is to model words and utterances in a hierarchical manner. Hierarchical approaches are widely applied to microblog sentiment and emotion classification. We gave a comprehensive overview of the hierarchical models in Section \ref{ham_hier}; in the current section we recap several recent methods, which inspired the choice of our model's architecture.

The core idea of hierarchical modeling is to create the representations of words, which are aggregated to create the representations for utterances; the latter are either used in the utterance-level inference or are further combined to make predictions on the conversational level. 

A number of related studies use BERT to create the contextual representations of words, which are then processed by the recurrent models to form the representations of utterances \cite{lei2019bert, ma2020han}. This approach is still not optimal because RNNs can not effectively capture the dependencies in the long input sequences and suffer from vanishing gradient. An alternative approach is to create utterance representations with Transformer \cite{shan2020contextual, li2020hierarchical}; our proposed architecture also follows this approach. In contrast to prior works, the distinguishing feature of our method is an effective way to overcome the limitation on the number of BERT input tokens. As opposed to cropping or processing single utterances out of context \cite{li2020hierarchical, shan2020contextual}, we process the whole input conversations in large chuncks, joining them into a unified context with Transformer.

Another way to utilize dialogue structure is to use a graph to represent the conversation. Such approaches are used to process multi-party conversations involving more than 2 speakers, which often have non-sequential structure, as a single utterance can have multiple responses to it. An intuitive way to model such conversations is to use utterances as the vertices in a dialogue graph; an edge will then connect the response to its parent utterance~\cite{hu2019gsn}. Alternatively one can exploit a fully-connected graph \cite{ghosal2019dialoguegcn}, under the assumption that all utterances influence each other. Graph-based modelling has proven to be effective on natural language tasks such as emotion classification \cite{zhang2019modeling, ghosal2019dialoguegcn}. However it is unnecessary for out setting, as we consider only dyadic dialogues, modeling utterances' interactions with Transformer. %Moreover, a complete graph can always be modeled with a Transformer, which we use in our method.

\paragraph{Speaker attribution.} Speaker attribution (the information which speaker the current utterance was produced by) is often used across various NLP tasks to create speakers' representations. For example, in utterance addressee identification \cite{le2019speaking, ouchi2016addressee} the models are trained to produce speakers' embeddings, which are explicitly used for addressee prediction. In other NLP tasks, such as sentiment classification or response selection, the learned speaker representations are blended into the model to enhance its performance.

To equip Transformer with speaker information, the studies by \citet{liu2021filling} and \citet{li2020hierarchical2} leverage specialized input masks to distinguish utterances from different speakers. These masks create distinct channels for each speaker in the encoder, so that an utterance representation can attend to the input from each speaker separately.

A simple but effective way to blend in speaker information into Transformer-based models, such as BERT, is to introduce additive speaker embeddings on the word level. For dyadic conversations, speakers are usually distinguished using BERT's segment embeddings~\cite{lu2020improving}; for the conversations with more than two speakers a common solution is to 
add a separate speaker embedding layer into BERT's embedding module \cite{gu2020speaker, yu-etal-2020-dialogue}.

PRIDE also incorporates speaker information; we add learned speaker embeddings to the Transformer input on the utterance level, following \citet{li2020hierarchical}, as well as using BERT's segment embeddings as speaker indicators on the word level.

\paragraph{Additional speaker information.} Knowing the speaker of each utterance enables to link the available external knowledge about that speaker, making the model's predictions more accurate. There has been significant research on creating response selection models infused with pre-defined speaker personality \cite{mazare2018training, zhang2018personalizing}. Such approaches operate on the utterance level, attaching personality to each generated utterance. As opposed to it, \citet{welch2019look} enriched the model for speaker attribute prediction on the global (conversation) level, adding various features of the interlocutor, such as relative age or gender. Inspired by this approach, we also enhance our model with the information about the speakers' ages.



