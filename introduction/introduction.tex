\section{Motivation and Problem Statement}
\label{mot}

\droppedcapital{T}{he} recent rise of social media enabled the access to vast amounts of user-generated content. This data has many potential applications; \textit{personalization} being a major use case.

Having background information about the user is practical for many downstream applications, such as recommender systems and search engines \cite{balog2019personal}. 
For instance, the ranking results for the query ``\textit{best weekend activities in Vancouver}'' can be rearranged based on the user's known hobbies to deliver more topical content for a particular individual. Another application greatly benefiting from the availability of the user's background is personalized intelligent assistants.

Chat-bots have become an essential part of everyday life, being able to deal with a wide range of routine tasks, provide factual information and entertain the user. Still, there is a growing demand for creating user-oriented intelligent agents, which are able to hold personalized conversations, while at the same time building and expanding their knowledge repository about the user's traits and preferences. 

Having access to the user's interests and background, personalized chat-bots will be able to build relevant responses and start new topics, interesting for the user. The ability to produce meaningful or even funny and surprising utterances is a desired feature, making a chat-bot appealing to the user. Therefore, there has been significant research interest in personalizing intelligent assistants, which still remains challenging. 

Consider, for example, the following conversation between a human (H) and an intelligent assistant (A), illustrating the need of the intelligent assistant to have background information about its interlocutor:
\begin{samepage}
\vspace{0.3cm} \\ \nopagebreak
\hspace*{0.3cm}{H:} {\em Can you text my wife I will be late from the hospital? Need to do an urgent operation.}\\\nopagebreak
\hspace*{0.3cm}{A:} {\em Sure.}\\\nopagebreak
\hspace*{0.3cm}{H:} {\em Any idea where I can get dinner that late?}\\\nopagebreak
\hspace*{0.3cm}{A:} {\em I'd suggest Ocean Thai Cafe, they are open till midnight.}\\\nopagebreak
\hspace*{0.3cm}{H:} {\em Sounds good! Talking about oceans, can you remind me to pack my goggles? Last time I went to the pool I forgot them.}
\vspace{0.3cm}
\end{samepage}

To effectively address the user's requests from the conversation above, the chat-bot has to know the following facts about the user: (i) \textit{interpersonal relationships} (knowing who the user's \textit{wife} is, to be able to contact her) and (ii) \textit{food preferences} (suggest an appropriate restaurant, knowing that the user loves \textit{thai} food).

To alleviate users from extensive manual effort to provide their information, the intelligent assistant should be capable of learning it directly from conversational utterances and digital traces of the user. For example, from the dialogue above the chat-bot can infer the user's profession \textit{surgeon} using the cues ``\textit{hospital}'' and ``\textit{operation}'' and the user's hobby \textit{swimming} from the words ``goggles'' and ``pool''. Such automatic extraction is an extremely challenging but interesting and useful task.

The work in this dissertation concerns predicting personal facts from textual representation of conversations. The sources of such data include transcribed everyday dialogues, social media submissions or chats in messenger apps. Information extraction from conversational data is a challenging task \cite{wu2020getting, yen2019personal}, which still is underexplored in related studies. 

\section{Personal Knowledge Base}

The format in which the personal information will be extracted, stored and used is an important issue to consider. Concerning this, we propose constructing a \textit{Personal Knowledge Base} (PKB) \cite{balog2019personal, yen2019personal}, a structured source of information about a particular individual. Such information could be the user's demographic facts, her interests, relationships or personal possessions.%, relationships to other entities (objects that they possess or people they communicate with).

Design and construction of PKBs has recently gained significant research interest \cite{vannur2020data, balog2019personal}. While some studies propose creating a PKB as a lifelog, consisting of a collection of life events \cite{yen2019personal, kao2021convlogminer}, our view of a PKB is analogous to the definition of a \textit{Personal Knowledge Graph} given by \citet{balog2019personal}, to be a structured collection of entities personally related to the user. 

We outline several properties of a PKB, which make it desirable to use in various applications:
\begin{itemize}
    \item \textit{Transferable:} PKB acts as a unique endpoint for storing and querying user information for a wide range of applications, regardless of their internal data representation.
    \item \textit{Well-structured:} the information in PKB should be stored in a consolidated and uniform format, for example, in the form of triplets $\langle$\textit{user}, \textit{attribute}, \textit{value}$\rangle$ (e.g., $\langle$\textit{user1}, \textit{hobby}, \textit{diving}$\rangle$), which makes searching for specific personal information easier.
    \item \textit{Owned by the user:} the users are provided with full control over their own personal knowledge, including revising stored facts and managing access by various services for personalization purposes.
    \item \textit{Interpretable:} explicit format of the personal facts in a PKB allows the users to have the explanations about any personalized decisions made by the applications accessing their PKB.
\end{itemize}

As noted in \citet{gerritse2020bias} PKBs can be involved in introducing bias based on specific personal traits. Indeed, while being a tool to enhance user experience and deliver relevant content, the information in a PKB, such as \textit{gender} or \textit{ethnicity} of the user, can lead to discrimination when used by personalized applications \cite{datta2015automated,  ali2019discrimination}. In this light, enabling the users to hide any sensitive facts in their PKB is a highly valuable feature.

One example of such storage of users' personal facts is Google Ads Settings\footnote{\href{https://adssettings.google.com}{https://adssettings.google.com}}. This service has some features of a PKB, such as storing explicit facts and allowing the users to modify them. However, the information in Ads Settings is not structured; for example, the preferences of the user are kept as a list of concepts, not separated into fine-grained categories (like \textit{favorite food} or \textit{preferred travel destination}). Also the information in Ads Settings is controlled by Google - collected from Google services and used for their ads, so that the user can not restrict the access for particular applications. %Our view of PKB is that the user is taking the ownership of it, listing the applications allowed to modify and access it.

\subsubsection{Personal Knowledge Base from Conversational Data}

As mentioned in the previous section, a PKB can be populated by automatically extracting personal facts from users' conversational data. Mining personal knowledge from user-generated content to populate PKBs, or \emph{user profiling}, is a long-standing topic in Natural Language Processing~\cite{flekova:ACL16:long,basile:2017}.

Creation of a PKB from conversational data requires addressing the following key issues:

\begin{itemize}
    \item Which personal facts are relevant and feasible to extract?
    \item How can this knowledge be inferred from conversational utterances?
    \item What are the potential applications of the data?
\end{itemize}

Concerning the first issue, we define personal facts to be user's demographic attributes (age, gender, origin, etc), hobbies and interests, interpersonal relationships (family status, names of friends, etc), skills, personality values or sentiments towards people and specific topics. Many of those attributes are subjective or mutable, making them specifically challenging to extract and process.

The second issue is concerned with information extraction from text. Prior works have mostly focused on well-comprehensible text genres,
such as Wikipedia articles or news stories, however, such methods do not work as well given conversations as input.
Compared to formal documents, dialogues are noisy, utterances are short \cite{bontcheva2014making}, the language used is
colloquial and the topics are diverse (including smalltalk). The dialogue utterances often give merely implicit cues about the speakers, making well established pattern-based extraction methods inapplicable.

Given that the personal information in conversational utterances is rarely stated explicitly, many prior studies opt for creating \textit{latent} user representations. We argue, however, that creating an explicit PKB containing distilled personal facts provides the following advantages:

\begin{itemize}
    \item The knowledge in a PKB can easily be shared among multiple applications, as it is not bound to some latent representation produced by a specific model. The stored facts can be reused and updated by any application, supporting the scenario of multiple repeated interactions with the user.
    \item A PKB is transparent and interpretable for the end users, providing them with full control over their personal knowledge, including revising stored facts and managing access of the downstream services for personalization purposes.
\end{itemize}

A detailed exploration of the third issue is beyond the scope of this thesis. We identify the potential applications of explicit personal facts to be personalized recommender systems, news feeds and content suggestions (for example, in video streaming services). Moreover, personal information can enhance the ranking results of search engines. Lastly, as motivated by the example in the previous section, the personal background knowledge can be used to produce topically focused and user-friendly responses by intelligent assistants.

\section{Task Formulation}

In our research we focus on inferring personal facts from conversational data. We work with crisp personal attributes, such as \textit{profession} or \textit{age}, ensuring that for all explored attributes we can define a finite list of possible values. In this work we do not consider subjective, changeable (\textit{sentiments}) and open-ended (\textit{favorite song}) attributes. We investigate personal attribute extraction from user-generated textual conversational data, such as transcribed spoken dialogues and social media submissions.

\section{State of the Art and its Limitations}

The related studies generally utilize pattern-based approaches, searching for explicit mentions of personal attributes in
speakers' utterances, such as extracting \textit{profession}: \textit{software engineer} and \textit{ employment\_history}: \textit{Microsoft} from \textit{``I work for Microsoft as a software engineer''} \cite{dial7}. Such methods are limited 
by their inability to consider implicit contexts (e.g., \textit{``I write product code in Redmond.''}), and routinely perform worse than methods based on inference.

Most inference-based methods predict personal attributes with a small set of values, often arranged in coarse-grained categories (e.g. predicting \textit{occupational class} instead of a fine-grained profession) or even modeled as a binary task (\textit{age}: \textit{young/old} \cite{liesenfeld2020predicting}, \textit{political orientation}: \textit{democrat/republican} \cite{preoctiuc2017beyond}). The inference of long-tailed personal attributes with a large number of values (like \textit{hobby} or \textit{favorite food type}), has mostly been overlooked in previous work.

On the other hand, there has been significant research effort on creating latent representations of speakers \cite{li2016persona, AIIDElin11}. Such representations can be directly used for response generation in a dialogue system \cite{zheng2019personalized}. Yet, such information is not scrutable, providing no possibility for the speaker to view and change it. Additionally, latent representations are difficult to transfer between models and applications.

\section{Contributions}

Within this dissertation we develop novel approaches for inferring personal knowledge from conversations, which address the limitations of the prior work. The contributions of this thesis can be summarized as follows. We introduce neural learning models tailored specifically for prediction of personal attributes:
\begin{itemize}
    \item \textbf{HAM}, a light-weight model for inferring demographic facts: \textit{gender}, \textit{age}, \textit{occupation} and \textit{family status}. HAM is based on attention mechanisms, allowing to inspect and interpret its predictions.
    \item \textbf{CHARM}, a zero-shot learning model for predicting long-tailed personal attributes, \textit{profession} and \textit{hobby}. CHARM can predict rare attribute values (such as \textit{hobby:curling}) without having training data for them.
    \item \textbf{PRIDE}, a transformer-based model for predicting directed fine-grained \textit{interpersonal relationships} of the speakers in dyadic conversations.
\end{itemize}

To support our experiments we create and release large-scale conversational datasets, labeled with personal attributes. Our datasets come in two flavours: conversational transcripts (movie and series scripts) and social media submissions (Reddit discussion threads). To the best of our knowledge, our datasets are the biggest and most comprehensive collections containing conversational data with personal attribute labels.

Finally, we conduct extensive experiments to show the viability of our models and their superior performance compared to the state-of-the-art baselines. We inspect the interpretability of the developed methods and perform stress tests in the transfer learning setup.

In summary, we provide a list of publications, from which this dissertation includes material:

\begin{itemize}
    \item 
    Tigunova, A., Yates, A., Mirza, P., & Weikum, G. (2019, May). \textbf{Listening between the lines: Learning personal attributes from conversations.} \textit{In The World Wide Web Conference} (pp. 1818-1828).
    \item
    Tigunova, A., Mirza, P., Yates, A., & Weikum, G. (2020, May). \textbf{RedDust: a Large Reusable Dataset of Reddit User Traits.} \textit{In Proceedings of the 12th Language Resources and Evaluation Conference} (pp. 6118-6126).
    \item
    Tigunova, A., Yates, A., Mirza, P., & Weikum, G. (2020, November). \textbf{CHARM: Inferring Personal Attributes from Conversations.} \textit{In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing} (pp. 5391-5404).
    \item
    Tigunova, A., Mirza, P., Yates, A., & Weikum, G. (2021, March). \textbf{Exploring Personal Knowledge Extraction from Conversations with CHARM.} \textit{In Proceedings of the 14th ACM International Conference on Web Search and Data Mining} (pp. 1077-1080).
    \item
    Tigunova, A., Mirza, P., Yates, A., & Weikum, G. (2021, November). \textbf{PRIDE: Predicting Relationships from Conversations.} \textit{In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing} (to appear).
\end{itemize}

Additionally the contents of Chapters \ref{chap_ham} and \ref{chap_charm} of this dissertation were presented at Doctoral Consortium in Web Conference 2020\footnote{Anna Tigunova. 2020. \textbf{Extracting Personal Information from Conversations.} In Companion Proceedings of the Web Conference 2020 (pp. 284â€“288).}.

\section{Organisation}

The rest of this thesis is organized as follows. In Chapter \ref{chap_backgr} we discuss relevant prior studies and give the necessary methodological background. Chapter \ref{chap_datasets} presents the conversational datasets we created to support the experiments in our work. In Chapters \ref{chap_ham}, \ref{chap_charm} and \ref{chap_pride} we describe our novel models for inferring personal attributes: HAM, CHARM and PRIDE, respectively. Finally, we conclude the dissertation in Chapter \ref{chap_conclusion} and outline possible directions for future research.