\section{Background}

In this section we elaborate on the methods for manual data labeling and evaluation of the obtained results. To annotate the datasets described in Section \ref{fire} we turn to crowdsourcing using MTurk online platform.

\textit{Crowdsourcing} is a powerful tool for getting annotations for large volumes of data at a low cost. Crowdsourcing is the process of solving a task by collecting and aggregating opinions from a group of people, called \textit{annotators}. Each annotator does not have to be an expert in the given task; a reasonable quality of the results is achieved by aggregating the annotations of a large number of people, e.g. using majority voting. The details on various answer aggregation approaches will be given in Section \ref{label_aggregation}.

\paragraphHd{Mturk crowdsourcing platform.} Amazon Mechanical Turk (\gls{MTurk}) is a crowdsourcing online tool, which enables researchers to publish surveys for data collection. The published Human Intelligence Tasks (HITs) are usually simple, so that they can be completed within a short time by a human annotator, yet HITs are hard for automated methods (for example, psychological surveys). Each completed HIT comes with metadata, such as the annotator's id or completion time, allowing to maintain the annotation quality by filtering out regularly underperforming workers. MTurk has proven to be a reliable tool, providing cheap data annotations \cite{peer2017beyond} used by researchers in many scientific areas. Crowdsourcing on MTurk allows the access to a more diverse demographics of annotators, as opposed to hiring live participants.

\paragraphHd{Inter-annotator agreement metrics.} Evaluating the reliability of crowdsourced annotations using inter-annotator agreement metrics
is an important step to ensure high quality of the collected labels. Additionally, calculating the degree of agreement allows to get an insight into the difficulty of the annotation task, which is helpful for modelling and refining the crowdsourcing assignments.

\textit{Fleiss' kappa} \cite{fleiss1971measuring} is a statistical inter-rater agreement measure applicable to the annotations for the multi-class classification problems. This metric allows any fixed number of annotators per item and evaluation of each item can be done by a different set of annotators. Fleiss' kappa computes the degree of the obtained inter-rater agreement over agreement expected by chance.

Let $N$ be the number of annotated pairs, indexed by $i=1,..N$, $K$ be the number of classification labels, indexed by $j=1,..K$, and $n_i_j$ be the number of annotators, who assigned $j$-th label to the $i$-th item. We first calculate the agreement of annotators per item $P_i$ and the proportion of items per label $p_j$: \vspace{-1cm}
\begin{spreadlines}{20pt}
\begin{gather*}
    P_i = \frac{1}{n (n-1)} \sum_{j=1}^K n_i_j(n_i_j - 1) \\  
    p_j = \frac{1}{\sum_{i=1}^K k_i} \sum_{i=1}^N n_i_j
\end{gather*}
\end{spreadlines}

Then Fleiss' kappa $\kappa$ is calculated as follows:
\begin{align}
    \kappa = \frac{\Tilde{P} - \Tilde{P_e}}{1 - \Tilde{P_e}}
    \label{eq1}
\end{align}

where
\begin{align} \Tilde{P} = \frac{1}{N} \sum_{i=1}^N P_i &&
    \Tilde{P_e} = \sum_{j=1}^k p_j^2
    \label{eq2}
\end{align}

The denominator in the equation for $\kappa$ shows the degree of agreement reachable above chance; the numerator shows the agreement that has actually been achieved. $\kappa = 1$ denotes perfect inter-rater agreement.

To account for the multi-label case we modified Fleiss' kappa calculations as follows: for each item $i=1,..N$ we denote $k_i$ to be  the number of labels, which were selected by at least one annotator for the item $i$. To calculate multi-label Fleiss' kappa we only have to change the equations for $P_i$ and $p_j$:

\begin{gather}
    P^{multi}_i = \frac{1}{k_i n (n-1)} \sum_{j=1}^K n_i_j(n_i_j - 1) \\
    p^{multi}_j = \frac{1}{\sum_{i=1}^K k_i} \sum_{i=1}^N n_i_j
\end{gather}

leaving the equations \ref{eq1}-\ref{eq2} the same as before. The single-label Fleiss kappa variant is used in Section \ref{kappa1} to evaluate \textit{profession} attribute labeling and in Section \ref{kappa2} to validate the precision of the automatic labeling approach; we use multi-label kappa in Section \ref{relatinoship_dataset} to estimate the agreement on labeling inter-speaker relationships.